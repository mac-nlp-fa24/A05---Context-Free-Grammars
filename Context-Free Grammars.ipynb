{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3995dac6-48d8-46b8-b709-0709eb434ef4",
   "metadata": {},
   "source": [
    "# Context-Free Grammars and Generative Grammars\n",
    "\n",
    "Learning Goals:\n",
    "You should be able to \n",
    "1. Understand and Explain the relationship between CFGs, the strings they accept and reject, and the parse trees that correspond to each accepted string.\n",
    "2. Generate valid strings top-down from a context free grammar.\n",
    "3. Design Context-Free Grammars that enforce particular grammatical rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24379bc-576c-461d-a974-a4b4bcfa97ff",
   "metadata": {},
   "source": [
    "### A Brief Review\n",
    "It's important to understand conceptually (and formally!) what a Context-Free Grammar is. A CFG consists of \n",
    "\n",
    "1. A *start symbol*, typically $S$ or another label that corresponds to the thing we want the grammar to generate. Since we are building models of sentences, $S$ represents a sentence. If you see any $TP$s or $IP$s or $CP$s, this is a sign that the person who wrote this grammar does fancy linguistics, but you can treat those like an $S$.\n",
    "\n",
    "2. A set of *terminals* $\\Sigma$, which, for our purposes, is identical to our Vocabulary. These represent the atomic units in the sentence we want to model.\n",
    "\n",
    "3. A set of *nonterminals* $N$, which are different kinds of high-level linguistic categories. These include standard parts of speech (nouns, verbs, adjectives, etc.) but also include terms that come from linguistic analysis, and let us understand chunks of a sentence larger than a token but smaller than the full sentence (which we call *constituents*). One example is a *Noun Phrases*, or $NP$, which let's us label chunks of a sentence larger than an individual noun like *dog* that still do the job of referring to that *dog* (i.e., we'd call *the dog* or *the red dog* or *the red dog who ran to the store* an NP!).\n",
    "\n",
    "4. A set of *production rules* R that map a non-terminal $A$ to a sequence of terminals and nonterminals $\\beta$. These tell us that our non-terminal $A$ *can consist of* the sequence $\\beta$. From the other direction, it means that we could assign the label $A$ to a sequence that looks like $\\beta$. And, most literally in the definition of a grammar, it means we can *replace* any symbol $A$ with the sequence $\\beta$ These relationships among terminals and nonterminals will be what lets us assign non-terminal labels to the sequences of terminals we start with!\n",
    "\n",
    "And the big idea is that a string is *grammatical* if (and only if) we can find some sequence of production rules to apply to our start symbol that generates that string!\n",
    "\n",
    "That's a lot of technical definition-y stuff, so let's get a bit more concrete and programmatic!\n",
    "\n",
    "## Generating from a Grammar\n",
    "\n",
    "Below is a templatic `Grammar` class that follows the technical definition fairly closely, with the exception being the set of rules `R`, which is structured as a dictionary mapping strings (what should be nonterminals!) to lists of tuples of strings. The `print_rules` method has been provided to print out the grammar in a format more consistent with your reading, and the second cell below instantiates the class with the grammar provided in your reading! \n",
    "\n",
    "You should...\n",
    "1. Reverse engineer an understanding of the representation of the grammar provided. Make sure you understand how it's structured and why it would be structured this particular way!\n",
    "3. **Your primary task:** Implement a method to generate a random sentence from a grammar. When the grammar provides multiple rules for a non-terminal, you should select one uniformly at random (recall the probability assignment and how to generate samples uniformly at random!). You may ignore the second return value for now.\n",
    "4. You'll notice that the generate method has `start_symbol` as a parameter (defaulting to the start symbol for our grammar). Allow for the grammar to generate a string that can be generated starting from *any* nonterminal in the grammar! If you want to be clever at this point, you may implement generate recursively.\n",
    "5. Now, make sure you understand what each non-terminal means. If you don't, a fun trick is `generate`-ing a few times from that nonterminal so you get an idea of what kinds of strings get assigned that label under the grammar!\n",
    "6. **Bonus, but good prep for next week!**: Since you're sampling at random from the grammar, consider how you'd assign a (log) probability to each string $s$ you generate from a nonterminal `start_symbol` $S'$, $\\log p(s \\mid S')$. That is, what is the probability you'd get the string you got given you gave `generate` the argument you did. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb0db1e-560e-4a3a-8a66-6aca01ea63dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Sequence, Mapping, Tuple\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Grammar:\n",
    "    def __init__(self,\n",
    "                 S : str, \n",
    "                 T : Iterable[str], \n",
    "                 NT : Iterable[str], \n",
    "                 R : Mapping[str, Sequence[Sequence[str]]]):\n",
    "        self.S = S\n",
    "        self.T = T\n",
    "        self.NT = NT\n",
    "        self.R = R\n",
    "\n",
    "    def print_rules(self) -> None:\n",
    "        for lhs, rhss in R.items():\n",
    "            for rhs in rhss:\n",
    "                print(\"{} -> {}\".format(lhs, \" \".join(rhs)))\n",
    "\n",
    "    def generate(self, start_symbol : str = None) -> Tuple[Sequence[str], float]:\n",
    "        if start_symbol == None:\n",
    "            start_symbol = self.S\n",
    "        #TODO Complete this method\n",
    "        return ([], -np.inf)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f0a0ef-83f5-4372-810f-f5f8fd6e8fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the J&M Reading\n",
    "\n",
    "S = \"S\"\n",
    "T = set([\"flights\", \"flight\", \"breeze\", \"trip\", \"morning\",\n",
    "     \"is\", \"prefer\", \"like\", \"need\", \"want\", \"fly\", \"do\",\n",
    "     \"non-stop\", \"first\", \"latest\", \"other\", \"direct\", \n",
    "     \"me\", \"I\", \"you\", \"it\",\n",
    "     \"Alaska\", \"Baltimore\", \"Los Angeles\", \"Chicago\", \"United\", \"American\",\n",
    "     \"the\", \"a\", \"an\", \"this\", \"these\", \"that\",\n",
    "     \"from\", \"to\", \"on\", \"near\", \"in\", \n",
    "     \"and\", \"or\", \"but\"])\n",
    "NT = set([\"N\", \"V\", \"Adj\", \"PRO\", \"PN\", \"D\", \"P\", \"Conj\"])\n",
    "R = {\n",
    "        # Lexical Rules\n",
    "        \"N\": [[\"flights\"], [\"flight\"], [\"breeze\"], [\"trip\"], [\"morning\"]],\n",
    "        \"V\": [[\"is\"], [\"prefer\"], [\"like\"], [\"need\"], [\"want\"], [\"fly\"], [\"do\"]],\n",
    "        \"Adj\": [[\"cheapest\"], [\"non-stop\"], [\"first\"], [\"latest\"], [\"other\"], [\"direct\"]],\n",
    "        \"PRO\": [[\"me\"], [\"you\"], [\"I\"], [\"it\"]],\n",
    "        \"PN\": [[\"Alaska\"], [\"Baltimore\"], [\"Los Angeles\"], [\"Chicago\"], [\"United\"], [\"American\"]],\n",
    "        \"D\": [[\"the\"], [\"a\"], [\"an\"], [\"this\"], [\"these\"]],\n",
    "        \"P\": [[\"from\"], [\"to\"], [\"on\"], [\"near\"], [\"in\"]],\n",
    "        \"Conj\": [[\"and\"], [\"or\"], [\"but\"]],\n",
    "        # Grammar Rules\n",
    "        \"S\":[[\"NP\", \"VP\"]],\n",
    "        \"NP\":[[\"PRO\"], \n",
    "              [\"PN\"], \n",
    "              [\"D\", \"Nom\"]],\n",
    "        \"Nom\":[[\"Nom\", \"N\"], \n",
    "               [\"N\"]],\n",
    "        \"VP\":[[\"V\"], \n",
    "              [\"V\", \"NP\"], \n",
    "              [\"V\", \"NP\", \"PP\"],\n",
    "              [\"V\", \"PP\"]],\n",
    "        \"PP\":[[\"P\", \"NP\"]]\n",
    "    }\n",
    "\n",
    "JMGrammar = Grammar(S, T, NT, R)\n",
    "JMGrammar.print_rules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594c284c-8b52-423e-9671-90559fa8d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(JMGrammar.generate()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1123c0f5-5361-4ebb-9987-0e0105666ef2",
   "metadata": {},
   "source": [
    "You may have noticed that your generations from this grammar can be a bit... nonsensical. Or, at the very least, what we as English speakers would call *ungrammatical*. Note that as we've discussed so far, there are two definitions of grammatical we're grappling with: One from our linguistics reading (a sentence a native speaker would judge as a good sentence of their language) and our new notion of grammatical, born from formal language theory: A sentence that can be generated by a given grammar. \n",
    "\n",
    "One goal of formal syntax is to bridge the gap between these two definitions: Find a way to construct a grammar such that every grammatical sentence to a native speaker of a language (in the linguistics sense) is grammatical with respect to that grammar (in the formal language sense). Of course, folks aren't typically doing this directly with Context-Free Grammars in linguistics departments (they've developed much more complex formalisms to describe grammars!), but they have given the problem we're running into a very special name: *overgeneration*. We've created a grammar that accepts the good sentences we want, but it is not constrained enough to reject bad sentences, which means sampling from the grammar can be an issue.\n",
    "\n",
    "**Bonus**: Try your best to restrict the grammar further in order to avoid some of the big mistakes the model is making *by modifying the grammar itself*. Avoid turning any currently grammatical sentences ungrammatical.\n",
    "\n",
    "For example, this grammar accepts sentences that don't obey very basic agreement constraints. We can fix this by replacing instances of $NP$ with $NP_{sg}$ and $NP_{pl}$, and $V$ with $V_{sg}$ and $V_{pl}$ such that the $_{sg}$ versions are singular and the $_{pl}$ versions are plural. The we enforce that $S$ can only be $NP_{sg} V_{sg}$ or $NP_{pl} V_{pl}$ (the $NP$ and $VP$ must match number!). We can also define $N_{sg}$ and $V_{sg}$ directly (we know which nouns and verbs are singular and plural!). The annoying part is constructing the right rules to map between the two!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0dad4ee2-43a3-48b9-9608-8f47a5f93ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Construct a new R here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
